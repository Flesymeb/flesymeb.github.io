<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Study on Blog</title><link>https://flesymeb.github.io/tags/study/</link><description>Recent content in Study on Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>© 2025 Hyoung Yan</copyright><lastBuildDate>Wed, 19 Feb 2025 19:14:32 +0800</lastBuildDate><atom:link href="https://flesymeb.github.io/tags/study/index.xml" rel="self" type="application/rss+xml"/><item><title>使用 LLaMAFactory 实现大模型微调</title><link>https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/</link><pubDate>Wed, 19 Feb 2025 19:14:32 +0800</pubDate><guid>https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/</guid><description>&lt;blockquote>
&lt;p>本文主要介绍如何使用 LLaMAFactory 实现大模型微调，基于 Qwen2.5-7B-Instruct 模型进行 LoRA 微调&lt;/p>
&lt;/blockquote>
&lt;h1 class="relative group">基础概念
&lt;div id="%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5" aria-label="锚点">#&lt;/a>
&lt;/span>
&lt;/h1>
&lt;h2 class="relative group">大模型训练流程
&lt;div id="%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
style="text-decoration-line: none !important;" href="#%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B" aria-label="锚点">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>&lt;strong>ChatGPT 的训练流程&lt;/strong>
&lt;figure>
&lt;img
class="my-0 rounded-md"
loading="lazy"
srcset="
/docs/study/20250219_qwen_fine_tune/imgs/01_hu7432936583427614731.png 330w,
/docs/study/20250219_qwen_fine_tune/imgs/01_hu6483044369201846705.png 660w,
/docs/study/20250219_qwen_fine_tune/imgs/01_hu10302658420192218926.png 1024w,
/docs/study/20250219_qwen_fine_tune/imgs/01_hu5206765031144962701.png 2x"
src="https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/imgs/01_hu6483044369201846705.png"
alt=""
/>
&lt;/figure>
图中的 Reward Modeling 和 Reinforcement Learning 可以看做一步，即 RLHF，因此训练一个大模型一般可以分为三步：&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/featured.png"/></item></channel></rss>