<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Study on Blog</title><link>https://flesymeb.github.io/tags/study/</link><description>Recent content in Study on Blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>© 2025 Hyoung Yan</copyright><lastBuildDate>Thu, 31 Jul 2025 18:42:29 +0800</lastBuildDate><atom:link href="https://flesymeb.github.io/tags/study/index.xml" rel="self" type="application/rss+xml"/><item><title>【科学计算】NumPy及其基本使用</title><link>https://flesymeb.github.io/docs/scientific-computing/numpy-study/</link><pubDate>Thu, 31 Jul 2025 18:42:29 +0800</pubDate><guid>https://flesymeb.github.io/docs/scientific-computing/numpy-study/</guid><description/></item><item><title>使用 LLaMAFactory 实现大模型微调</title><link>https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/</link><pubDate>Wed, 19 Feb 2025 19:14:32 +0800</pubDate><guid>https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/</guid><description>&lt;blockquote>
&lt;p>本文主要介绍如何使用 LLaMAFactory 实现大模型微调，基于 Qwen2.5-7B-Instruct 模型进行 LoRA 微调&lt;/p>
&lt;/blockquote>
&lt;h1 class="relative group">基础概念
&lt;div id="基础概念" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#%e5%9f%ba%e7%a1%80%e6%a6%82%e5%bf%b5" aria-label="锚点">#&lt;/a>
&lt;/span>
&lt;/h1>
&lt;h2 class="relative group">大模型训练流程
&lt;div id="大模型训练流程" class="anchor">&lt;/div>
&lt;span
class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
&lt;a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700 !no-underline" href="#%e5%a4%a7%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83%e6%b5%81%e7%a8%8b" aria-label="锚点">#&lt;/a>
&lt;/span>
&lt;/h2>
&lt;p>&lt;strong>ChatGPT 的训练流程&lt;/strong>
&lt;figure>
&lt;img
class="my-0 rounded-md"
loading="lazy"
decoding="async"
fetchpriority="low"
alt=""
srcset="
/docs/study/20250219_qwen_fine_tune/imgs/01_hu7432936583427614731.png 330w,
/docs/study/20250219_qwen_fine_tune/imgs/01_hu6483044369201846705.png 660w,
/docs/study/20250219_qwen_fine_tune/imgs/01_hu7176193833438702269.png 1280w
"
data-zoom-src="https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/imgs/01.png"
src="https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/imgs/01.png">
&lt;/figure>
图中的 Reward Modeling 和 Reinforcement Learning 可以看做一步，即 RLHF，因此训练一个大模型一般可以分为三步：&lt;/p></description><media:content xmlns:media="http://search.yahoo.com/mrss/" url="https://flesymeb.github.io/docs/study/20250219_qwen_fine_tune/featured.png"/></item></channel></rss>